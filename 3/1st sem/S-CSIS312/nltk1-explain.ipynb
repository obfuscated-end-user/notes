{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31bac4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"ds/SMSSpamCollection\", sep=\"\\t\", header=None)\n",
    "data.columns = [\"label\", \"msg\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4337a2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>msg_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                           msg_clean  \n",
       "0  go jurong point crazi avail bugi n great world...  \n",
       "1                              ok lar joke wif u oni  \n",
       "2  free entri 2 wkli comp win fa cup final tkt 21...  \n",
       "3                u dun say earli hor u c alreadi say  \n",
       "4        nah i dont think goe usf live around though  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join([c for c in txt if c not in string.punctuation])\n",
    "    tokens = re.split(\"\\W+\", txt)\n",
    "    txt = \" \".join([ps.stem(word) for word in tokens if word not in stopwords])\n",
    "    return txt\n",
    "\n",
    "data[\"msg_clean\"] = data[\"msg\"].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f3d0fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape\n",
      " (3, 14)\n",
      "X\n",
      "   (0, 11)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 7)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 2)\t1\n",
      "X.toarray()\n",
      " [[0 0 0 0 0 0 1 1 1 0 0 1 0 1]\n",
      " [1 0 0 1 1 0 0 0 0 0 0 1 1 0]\n",
      " [0 1 1 0 0 1 0 0 0 1 1 0 0 0]]\n",
      "df\n",
      "    another sentence  document is  document is here  is another  \\\n",
      "0                 0            0                 0           0   \n",
      "1                 1            0                 0           1   \n",
      "2                 0            1                 1           0   \n",
      "\n",
      "   is another sentence  is here  is sentence  is sentence is  sentence is  \\\n",
      "0                    0        0            1               1            1   \n",
      "1                    1        0            0               0            0   \n",
      "2                    0        1            0               0            0   \n",
      "\n",
      "   third document  third document is  this is  this is another  \\\n",
      "0               0                  0        1                0   \n",
      "1               0                  0        1                1   \n",
      "2               1                  1        0                0   \n",
      "\n",
      "   this is sentence  \n",
      "0                 1  \n",
      "1                 0  \n",
      "2                 0  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "page 29 on ppt\n",
    "1. Explain the result of the data.\n",
    "2. Try to change by bi gram and tri gram (2, 3) observe and explain the results.\n",
    "\n",
    "Explanation:\n",
    "1.\n",
    "CountVectorizer counts how much an n-gram appears in a document (in this case, for each string in the corpus).\n",
    "N-grams are a collection of n successive items in a text document that may include words, numbers, symbols, and punctuation.\n",
    "The n-gram range of (2, 2) means that find chunks of text that are 2 words long at minimum, and 2 words long at maximum. This will only find chunks of text that are excatly 2 words long. An n-gram range of (2, 3) would mean 2 words long at minimum, and 3 words at maximum, meaning that it would find BOTH 2- and 3-word long chunks.\n",
    "\n",
    "fit() computes the mean and std to be used for later scaling.\n",
    "transform() performs standardization by centering and scaling.\n",
    "fit_transform() combines these two operations.\n",
    "\n",
    "a. X.shape is the shape of the corpus fitted into the CharacterVectorizer object. In this case, it is (3, 8) because the corpus has 3 sentences each, and 8 because there are 8 n-grams that are generated from the function.\n",
    "b. Printing the X variable itself shows the indices where n-grams with the value 1 are located. For example, typing X[0, 11] will return 1 because that n-gram exists for a specific sentence. X[0, 10] will return 0 because that n-gram is not present in a specific sentence.\n",
    "c. X.toarray() converts the corpus into an array composed of 1s and 0s. 1 means that a certain n-gram (column) is present in that sentence (row), and 0 means the that the specific n-gram is not present in anywhere in a given sentence.\n",
    "d. The DataFrame at the end provides a visualization on what n-grams (columns) correspond to what sentence (row), in contrast to the X.toarray() method earlier that just shows the array of 1s and 0s. The columns are arranged in alphabetical order, and is also the same order as the toarrays() method from above.\n",
    "\n",
    "2.\n",
    "Changing the ngram_range from (2, 2) to (2, 3) made the CountVectorizer capture n-grams that are 2 or 3 words long, in contrast to the other interpretation beforehand, which only yielded 2-word results.\n",
    "\n",
    "some quickies:\n",
    "1. if countvectorizer is initialized with no params, n-gram range is (1, 1)\n",
    "2. countvectorizer counts how much a word/n-gram appears in a document (in this case, for each string in the corpus)\n",
    "3. array size is determined by the number of sentences in the corpus, by the feature names (use get_feature_names_out()) (3x14)\n",
    "4. 1 means that certain n-gram is present in the document, while 0 means it's not. for example, \"sentence is\" is present in corpus[0], but \"document is\" is not.\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(2, 3)) # changed from (2, 2) to (2, 3)\n",
    "\n",
    "corpus = [\"This is a sentence is\",\n",
    "          \"This is another sentence\",\n",
    "          \"third document is here\"]\n",
    "\n",
    "X = cv.fit_transform(corpus)\n",
    "print(X.shape)\n",
    "print(X)\n",
    "print(X.toarray())\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out()) # get_feature_names() doesn't work\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4730ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
