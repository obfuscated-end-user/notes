{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D8V1Tz1Hqhw8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    {\"text\": \"Hello, how are you?\", \"intent\": \"greeting\"},\n",
        "    {\"text\": \"What's your name?\", \"intent\": \"question\"},\n",
        "    {\"text\": \"I'm feeling sad today.\", \"intent\": \"emotion\"},\n",
        "    {\"text\": \"Can you tell me a joke?\", \"intent\": \"entertainment\"},\n",
        "    {\"text\": \"Goodbye, see you later!\", \"intent\": \"greeting\"},\n",
        "    {\"text\": \"How old are you?\", \"intent\": \"question\"},\n",
        "    {\"text\": \"I'm feeling happy today.\", \"intent\": \"emotion\"},\n",
        "    {\"text\": \"Can you recommend a movie?\", \"intent\": \"entertainment\"},\n",
        "    {\"text\": \"I want to book a flight from New York to Los Angeles.\", \"intent\": \"flight_booking\"}\n",
        "]\n",
        "\n",
        "intent_to_label = {\n",
        "    \"greeting\": 0,\n",
        "    \"question\": 1,\n",
        "    \"emotion\": 2,\n",
        "    \"entertainment\": 3,\n",
        "    \"flight_booking\": 4\n",
        "}\n",
        "\n",
        "labels = []\n",
        "for example in dataset:\n",
        "    labels.append(intent_to_label[example[\"intent\"]])\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split([example[\"text\"] for example in dataset], labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "_CYCn8r_ql2E"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "\n",
        "train_input_ids = torch.tensor(train_encodings['input_ids'])\n",
        "train_attention_mask = torch.tensor(train_encodings['attention_mask'])\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "val_input_ids = torch.tensor(val_encodings['input_ids'])\n",
        "val_attention_mask = torch.tensor(val_encodings['attention_mask'])\n",
        "val_labels = torch.tensor(val_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaJpasJYqxEt",
        "outputId": "ba219f5e-0c6a-47d0-e7d0-a99557571b98"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-33-e48521c59473>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_labels = torch.tensor(train_labels)\n",
            "<ipython-input-33-e48521c59473>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_labels = torch.tensor(val_labels)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "jsSWbXMHq296"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ConversationalDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "val_dataset = ConversationalDataset(val_input_ids, val_attention_mask, val_labels)\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "zQ7BHM8xq9Zb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"epoch {epoch+1}, loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_correct = 0\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            _, predicted = torch.max(outputs.logits, dim=1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "ner_model = pipeline(\"ner\", model=\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLdhjAZArFnO",
        "outputId": "92587a4b-624f-4737-b9b8-25247695ad21"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, loss: 1.2405967712402344\n",
            "epoch 2, loss: 1.1747289896011353\n",
            "epoch 3, loss: 1.2043672800064087\n",
            "epoch 4, loss: 1.1451902389526367\n",
            "epoch 5, loss: 1.0798234939575195\n",
            "epoch 6, loss: 1.1614047288894653\n",
            "epoch 7, loss: 1.1084938049316406\n",
            "epoch 8, loss: 0.9849572777748108\n",
            "epoch 9, loss: 1.0009880065917969\n",
            "epoch 10, loss: 1.0408557653427124\n",
            "epoch 11, loss: 0.9276723265647888\n",
            "epoch 12, loss: 0.8964638113975525\n",
            "epoch 13, loss: 0.9480992555618286\n",
            "epoch 14, loss: 0.8420810103416443\n",
            "epoch 15, loss: 0.8750584721565247\n",
            "epoch 16, loss: 0.8209642171859741\n",
            "epoch 17, loss: 0.8008236885070801\n",
            "epoch 18, loss: 0.7374863028526306\n",
            "epoch 19, loss: 0.8163710832595825\n",
            "epoch 20, loss: 0.8957806825637817\n",
            "epoch 21, loss: 0.8373875021934509\n",
            "epoch 22, loss: 0.7354558706283569\n",
            "epoch 23, loss: 0.8144745826721191\n",
            "epoch 24, loss: 0.6359387636184692\n",
            "epoch 25, loss: 0.6128525733947754\n",
            "epoch 26, loss: 0.6850585341453552\n",
            "epoch 27, loss: 0.6907614469528198\n",
            "epoch 28, loss: 0.6368293762207031\n",
            "epoch 29, loss: 0.7075925469398499\n",
            "epoch 30, loss: 0.670913815498352\n",
            "epoch 31, loss: 0.5955477952957153\n",
            "epoch 32, loss: 0.5724506378173828\n",
            "epoch 33, loss: 0.5716348886489868\n",
            "epoch 34, loss: 0.5714108347892761\n",
            "epoch 35, loss: 0.5948861241340637\n",
            "epoch 36, loss: 0.5660225749015808\n",
            "epoch 37, loss: 0.5139713883399963\n",
            "epoch 38, loss: 0.49322953820228577\n",
            "epoch 39, loss: 0.47632166743278503\n",
            "epoch 40, loss: 0.4554351270198822\n",
            "epoch 41, loss: 0.44286948442459106\n",
            "epoch 42, loss: 0.498189240694046\n",
            "epoch 43, loss: 0.4640907645225525\n",
            "epoch 44, loss: 0.42398136854171753\n",
            "epoch 45, loss: 0.4792085289955139\n",
            "epoch 46, loss: 0.41855496168136597\n",
            "epoch 47, loss: 0.4418127238750458\n",
            "epoch 48, loss: 0.4353165030479431\n",
            "epoch 49, loss: 0.42349356412887573\n",
            "epoch 50, loss: 0.42064884305000305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(text):\n",
        "    entities = ner_model(text)\n",
        "    entity_list = [(entity[\"word\"], entity[\"score\"], entity[\"entity\"]) for entity in entities]\n",
        "    return entity_list\n",
        "\n",
        "def recognize_intent(text):\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = encoding[\"input_ids\"].flatten()\n",
        "    attention_mask = encoding[\"attention_mask\"].flatten()\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    outputs = model(input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))\n",
        "    _, predicted = torch.max(outputs.logits, dim=1)\n",
        "    intent = list(intent_to_label.keys())[list(intent_to_label.values()).index(predicted.item())]\n",
        "    return intent"
      ],
      "metadata": {
        "id": "zE_yXwOorNFF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Say something: \")\n",
        "entities = extract_entities(text.lower())\n",
        "print(\"Entities:\", entities)\n",
        "\n",
        "intent = recognize_intent(text)\n",
        "print(\"Intent:\", intent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZNrhK1-rUsW",
        "outputId": "567b46b7-701f-4b72-8d6a-3106b041bd85"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Say something: I want to book a flight from New York to Vancouver.\n",
            "Entities: [('i', 0.6763418, 'LABEL_0'), ('want', 0.6037237, 'LABEL_0'), ('to', 0.63456213, 'LABEL_0'), ('book', 0.5630288, 'LABEL_0'), ('a', 0.54782164, 'LABEL_0'), ('flight', 0.66510147, 'LABEL_0'), ('from', 0.5150968, 'LABEL_0'), ('new', 0.72696835, 'LABEL_0'), ('york', 0.64996743, 'LABEL_0'), ('to', 0.5500471, 'LABEL_0'), ('vancouver', 0.60047096, 'LABEL_0'), ('.', 0.57571155, 'LABEL_0')]\n",
            "Intent: flight_booking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8q6nVM9ArW8U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}