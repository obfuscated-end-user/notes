{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e77520c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/patrickloeber/pytorchTutorial/blob/master/03_autograd.py\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3076a786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.2765, 1.6360, 1.0300], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The autograd package provides automatic differentiation\n",
    "# for all operations on Tensors\n",
    "\n",
    "# requires_grad = True -> tracks all operations on the tensor.\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43447b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2765, -0.3640, -0.9700], requires_grad=True)\n",
      "tensor([3.2765, 1.6360, 1.0300], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x000002A93A3BAD40>\n"
     ]
    }
   ],
   "source": [
    "# y was created as a result of an operation, s it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x) # created by the user -> grad_fn is Non\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed7f357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32.2070,  8.0299,  3.1824], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14.4731, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z = z.mean()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6726830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.5531, 3.2721, 2.0599])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's compute the gradients with backpropagation\n",
    "# When we finish our computation we can call .backward() and have all the graidents computed automatically.\n",
    "# The gradient for this tensor will be accumulated into .grad attribute\n",
    "# It is the partial derivate of the function w.r.t. the tensor\n",
    "\n",
    "z.backward()\n",
    "x.grad # dz/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cca87bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  599.3216, -1282.8489,   319.4081], grad_fn=<MulBackward0>)\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n",
    "# It computes partial derviates while applyting the chain rule\n",
    "\n",
    "# Model with non-scalar output:\n",
    "# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward()\n",
    "# specify a gradient argument that is a tensor of matching shape.\n",
    "# needed for vector-Jacobian product\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "for _ in range(10):\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
    "y.backward(v)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e51af46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "True\n",
      "<SumBackward0 object at 0x000002A9426D5840>\n"
     ]
    }
   ],
   "source": [
    "# Stop a tensor from tracking history:\n",
    "# For example during our training loop when we want to update our weights\n",
    "# then this update operation should not be part of the gradient computation\n",
    "# - x.requires_grad_(False)\n",
    "# - x.detach()\n",
    "# - wrap in 'with torch.no_grad():'\n",
    "\n",
    "# .requires_grad_(...) hcanges an existing flag in-place.\n",
    "a = torch.randn(2, 2)\n",
    "print(a.requires_grad)\n",
    "b = ((a * 3) / (a - 1))\n",
    "print(b.grad_fn)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e414c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafbac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n",
      "tensor(4.8000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "# !!! We need to be careful during optimization !!!\n",
    "# Use .zero_() to empty the gradients before a new optimization step!\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    # just a dummy example\n",
    "    model_output = (weights * 3).sum()\n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "    \n",
    "    # optimize model, i.e. adjust weights...\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "        \n",
    "    # this is important! It affects the final weights & output\n",
    "    weights.grad.zero_()\n",
    "\n",
    "print(weights)\n",
    "print(model_output)\n",
    "\n",
    "# Optimizer has zero_grad() method\n",
    "# optimizer = torch.optim.SGC([weights], lr=0.1)\n",
    "# During training:\n",
    "# optimizer.step()\n",
    "# optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455f2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
